# identify_cats-dogs
大一萌新的第一次尝试，[训练图片参考这个链接](https://rec.ustc.edu.cn/share/38b85c40-c925-11ec-9cb1-89ae9e4b462c) ,访问密码ustc1958.基本也是对着一些博客的代码修修补补完成科学研讨的课题。初步了解了深度学习领域的一些问题，在此实现了利用Alexnet网络识别猫狗的功能。

## 卷积神经网络的训练过程

>  第一个阶段是数据由低层次向高层次传播的阶段，即前向传播阶段。

> 第二个阶段是，当前向传播得出的结果与预期不相符时，将误差从高层次向底层次进行传播训练的阶段，即反向传播阶段。

> 训练过程为：
1. 网络进行权值的初始化；
2. 输入数据经过卷积层、下采样层、全连接层的向前传播得到输出值；
3. 求出网络的输出值与目标值之间的误差；
4. 当误差大于我们的期望值时，将误差传回网络中，依次求得全连接层，下采样层，卷积层的误差。各层的误差可以理解为对于网络的总误差，网络应承担多少；当误差等于或小于我们的期望值时，结束训练。
5. 根据求得误差进行权值更新。然后在进入到第二步。

### 向前传播过程

可参考这个博客 ：http://www.woshipm.com/ai/3884563.html

1. 数据规则化	彩色图像的输入通常先要分解为R（红）G（绿）B（蓝）三个通道，其中每个值介于0~255之间。
2. 卷积运算（Convolution）
3. 激活	此处使用的激活函数是Relu函数。
4. 池化（Pooling）	池化的目的是提取特征，减少向下一个阶段传递的数据量。
5. 全连接（Fully-connected layer）	对卷积结果展开

## Alexnet网络

#### **优点**:

**AlexNet将CNN用到了更深更宽的网络中,相比于以前的LeNet其效果分类的精度更高**

1. ReLU的应用

2. Dropout随机失活    随机忽略一些神经元,以避免过拟合。

3. 重叠的最大池化层

4. 提出了LRN（Local Response Normalization）层

   局部响应归一化,对局部神经元创建了竞争的机制,使得其中响应小的值变得更大,并抑制反馈较小的。

5. 数据增强（data augmentation）

   

   #### 结构

   <img src="https://github.com/poetrilin/identify_cats-dogs/blob/poetrilin-patch-1/alexnet.jpg"/>


   > 第一层：卷积层1，输入为 224 × 224 × 3 224 \times 224 \times 3224×224×3的图像，卷积核的数量为96，论文中两片GPU分别计算48个核; 卷积核的大小为 11 × 11 × 3 11 \times 11 \times 311×11×3; stride = 4, stride表示的是步长， pad = 0, 表示不扩充边缘;

   > 第二层：卷积层2, 输入为上一层卷积的feature map， 卷积的个数为256个，论文中的两个GPU分别有128个卷积核。卷积核的大小为：5 × 5 × 48 5 \times 5 \times 485×5×48; pad = 2, stride = 1; 然后做 LRN， 最后 max_pooling, pool_size = (3, 3), stride = 2;

   > 第三层：卷积3, 输入为第二层的输出，卷积核个数为384, kernel_size = (3 × 3 × 256 3 \times 3 \times 2563×3×256)， padding = 1, 第三层没有做LRN和Pool

   > 第四层：卷积4, 输入为第三层的输出，卷积核个数为384, kernel_size = (3 × 3 3 \times 33×3), padding = 1, 和第三层一样，没有LRN和Pool

   > 第五层：卷积5, 输入为第四层的输出，卷积核个数为256, kernel_size = (3 × 3 3 \times 33×3), padding = 1。然后直接进行max_pooling, pool_size = (3, 3), stride = 2;

   > 第6,7,8层是全连接层，每一层的神经元的个数为4096，最终输出softmax为1000,因为上面介绍过，ImageNet这个比赛的分类个数为1000。全连接层中使用了RELU和Dropout。

